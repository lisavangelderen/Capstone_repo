{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "saving-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latin-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "MAX_MEMORY = 1000000\n",
    "BATCH_SIZE = 20\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Solver starts\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ScoreEvaluator:\n",
    "\n",
    "    def __init__(self, max_len, average_of_last_runs, model = None):\n",
    "        self.max_len = max_len\n",
    "        self.score_table = deque(maxlen=self.max_len)\n",
    "        self.model = model\n",
    "        self.average_of_last_runs = average_of_last_runs\n",
    "\n",
    "    def store_score(self, episode, step):\n",
    "        self.score_table.append([episode, step])\n",
    "\n",
    "    def evaluation_score(self, title = \"Training\"):\n",
    "        return self.score_table\n",
    "        \n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.exploration_rate = 1.0\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(self.action_space, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(0, self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "            for state, action, reward, state_next, done in minibatch:\n",
    "                Q = reward\n",
    "                if not done:\n",
    "                    Q = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "                Q_values = self.model.predict(state)\n",
    "                Q_values[0][action] = Q\n",
    "                self.model.fit(state, Q_values, verbose=0)\n",
    "            self.exploration_rate *= EXPLORATION_DECAY\n",
    "            self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class TrainSolver:\n",
    "\n",
    "    def __init__(self, max_episodes):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.score_table = deque(maxlen=400)\n",
    "        self.average_of_last_runs = None\n",
    "        self.model = None\n",
    "        self.play_episodes = 100\n",
    "        env = gym.make(\"maze-random-10x10-plus-v0\")\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        self.solver = Network(observation_space, action_space)\n",
    "\n",
    "    def train(self):\n",
    "        env = gym.make(\"maze-random-10x10-plus-v0\")\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Solver starts\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "        self.model = self.solver.get_model()\n",
    "        episode = 0\n",
    "        score_eval = ScoreEvaluator(400, 50, self.model)\n",
    "        while episode < self.max_episodes:\n",
    "\n",
    "            episode += 1\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, observation_space])\n",
    "            step = 0\n",
    "            while True:\n",
    "\n",
    "                step += 1\n",
    "                action = self.solver.take_action(state)\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "                if not done:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -reward\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "                self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print(\"Run: \" + str(episode) + \", exploration: \" + str(self.solver.exploration_rate) + \", score: \" + str(step))\n",
    "                    self.score_table.append([episode, step])\n",
    "                    score_eval.store_score(episode, step)\n",
    "                    break\n",
    "                self.solver.experience_replay()\n",
    "        \n",
    "        train_scores = score_eval.evaluation_score(\"100 trains\")\n",
    "        with open(\"Cartpole_DQN_train_scores.csv\", \"w\") as csvfile:\n",
    "            header = [\"episode\", \"score\"]\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            for i in train_scores:\n",
    "                i = tuple(i)\n",
    "                episode, score = i[0], i[1]\n",
    "                writer.writerow([episode, score])\n",
    "\n",
    "    def return_trained_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def play(self, play_episodes=100, load_model=False, model_weights_dir=None, trained_model=None):\n",
    "\n",
    "        self.play_episodes = play_episodes\n",
    "        if load_model is not False:\n",
    "            if model_weights_dir is None:\n",
    "                print(\"Can't load specified model\")\n",
    "            elif trained_model is None:\n",
    "                print(\"Please pass a valid model as a parameter\")\n",
    "            else:\n",
    "                model = trained_model\n",
    "                model.load(model_weights_dir)\n",
    "        else:\n",
    "            model = self.model\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        episode = 0\n",
    "        score_eval = ScoreEvaluator(400, 100, model)\n",
    "        while episode < self.play_episodes:\n",
    "\n",
    "            episode += 1\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, observation_space])\n",
    "            step = 0\n",
    "            while True:\n",
    "\n",
    "                step += 1\n",
    "                action = self.solver.take_action(state)\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "\n",
    "                if not done:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -reward\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "                self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print(\"Run: \" + str(episode) + \", score: \" + str(\n",
    "                        step))\n",
    "                    self.score_table.append([episode, step])\n",
    "                    score_eval.store_score(episode, step)\n",
    "                    break\n",
    "                self.solver.experience_replay()\n",
    "        \n",
    "        play_scores = score_eval.evaluation_score(\"100 Plays\")\n",
    "        with open(\"Cartpole_DQN_play_scores.csv\", \"w\") as csvfile:\n",
    "            header = [\"episode\", \"score\"]\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            for i in play_scores:\n",
    "                i = tuple(i)\n",
    "                episode, score = i[0], i[1]\n",
    "                writer.writerow([episode, score])\n",
    "                \n",
    "    def save_model(self):\n",
    "        self.model.save('cartpole_dqn_model.h5')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = TrainSolver(1)\n",
    "    trainer.train()\n",
    "    trainer.play(1)\n",
    "    trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-brazilian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
