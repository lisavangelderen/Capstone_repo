{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "naked-transportation",
   "metadata": {},
   "source": [
    "The (adapted) code in this notebook comes from: https://github.com/siddharthkale97/CartPole-v1-DeepQ-AI/blob/master/DeepQcartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "future-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import csv\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suspected-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "MAX_MEMORY = 1000000\n",
    "BATCH_SIZE = 20\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dated-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreEvaluator:\n",
    "\n",
    "    def __init__(self, max_len, average_of_last_runs, model = None):\n",
    "        self.max_len = max_len\n",
    "        self.score_table = deque(maxlen=self.max_len)\n",
    "        self.model = model\n",
    "        self.average_of_last_runs = average_of_last_runs\n",
    "\n",
    "    def store_score(self, episode, step):\n",
    "        self.score_table.append([episode, step])\n",
    "\n",
    "    def evaluation_score(self, title = \"Training\"):\n",
    "        return self.score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "egyptian-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.exploration_rate = 1.0\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(self.action_space, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            a = random.randrange(0, self.action_space)\n",
    "            return a, True\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0]), False\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "            for state, action, reward, state_next, done in minibatch:\n",
    "                Q = reward\n",
    "                if not done:\n",
    "                    Q = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "                Q_values = self.model.predict(state)\n",
    "                Q_values[0][action] = Q\n",
    "                self.model.fit(state, Q_values, verbose=0)\n",
    "            self.exploration_rate *= EXPLORATION_DECAY\n",
    "            self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "saved-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Solver starts\n",
      "---------------------------------\n",
      "Run: 1, exploration: 1.0, score: 9\n",
      "Run: 2, exploration: 1.0, score: 9\n",
      "Run: 3, exploration: 0.9655206468094844, score: 9\n",
      "Run: 4, exploration: 0.918316468354365, score: 11\n",
      "Run: 5, exploration: 0.8778091417340573, score: 10\n",
      "Run: 6, exploration: 0.8433051360508336, score: 9\n",
      "Run: 7, exploration: 0.810157377815473, score: 9\n",
      "Run: 8, exploration: 0.7744209942832988, score: 10\n",
      "Run: 9, exploration: 0.7439808620067382, score: 9\n",
      "Run: 10, exploration: 0.7076077347272662, score: 11\n",
      "Run: 11, exploration: 0.6763948591909945, score: 10\n",
      "Run: 12, exploration: 0.5997278763867329, score: 25\n",
      "Run: 13, exploration: 0.5732736268885887, score: 10\n",
      "Run: 14, exploration: 0.547986285490042, score: 10\n",
      "Run: 15, exploration: 0.5238143793828016, score: 10\n",
      "Run: 16, exploration: 0.500708706245853, score: 10\n",
      "Run: 17, exploration: 0.47862223409330756, score: 10\n",
      "Run: 18, exploration: 0.4598090507939749, score: 9\n",
      "Run: 19, exploration: 0.43952667968844233, score: 10\n",
      "Run: 20, exploration: 0.4222502236424958, score: 9\n",
      "Run: 21, exploration: 0.4036245882390106, score: 10\n",
      "Run: 22, exploration: 0.38389143477919885, score: 11\n",
      "Run: 23, exploration: 0.3706551064126331, score: 8\n",
      "Run: 24, exploration: 0.3543053533848483, score: 10\n",
      "Run: 25, exploration: 0.3403786882983606, score: 9\n",
      "Run: 26, exploration: 0.3125753549412418, score: 18\n",
      "Run: 27, exploration: 0.30028896908517405, score: 9\n",
      "Run: 28, exploration: 0.28993519966087045, score: 8\n",
      "Run: 29, exploration: 0.27714603575484437, score: 10\n",
      "Run: 30, exploration: 0.26759021970270175, score: 8\n",
      "Run: 31, exploration: 0.2532352299289372, score: 12\n",
      "Run: 32, exploration: 0.24328132378095624, score: 9\n",
      "Run: 33, exploration: 0.23371867538818816, score: 9\n",
      "Run: 34, exploration: 0.22340924607110255, score: 10\n",
      "Run: 35, exploration: 0.21248679717794605, score: 11\n",
      "Run: 36, exploration: 0.2041345879004775, score: 9\n",
      "Run: 37, exploration: 0.19611067854912728, score: 9\n",
      "Run: 38, exploration: 0.18840216465300522, score: 9\n",
      "Run: 39, exploration: 0.18099664897669618, score: 9\n",
      "Run: 40, exploration: 0.17388222158237718, score: 9\n",
      "Run: 41, exploration: 0.16704744067563246, score: 9\n",
      "Run: 42, exploration: 0.15888051309497406, score: 11\n",
      "Run: 43, exploration: 0.1518722266715875, score: 10\n",
      "Run: 44, exploration: 0.14590259167570602, score: 9\n",
      "Run: 45, exploration: 0.14016760486247823, score: 9\n",
      "Run: 46, exploration: 0.1255322834371622, score: 23\n",
      "Run: 47, exploration: 0.11999500148501063, score: 10\n",
      "Run: 48, exploration: 0.1135578192100607, score: 12\n",
      "Run: 49, exploration: 0.10376060541355137, score: 19\n",
      "Run: 50, exploration: 0.09918368135888474, score: 10\n",
      "Run: 51, exploration: 0.08750185146499175, score: 26\n",
      "Run: 52, exploration: 0.08364210790237678, score: 10\n",
      "Run: 53, exploration: 0.08035439121179945, score: 9\n",
      "Run: 54, exploration: 0.0716045256805401, score: 24\n",
      "Run: 55, exploration: 0.061607401965086545, score: 31\n",
      "Run: 56, exploration: 0.05918580250061433, score: 9\n",
      "Run: 57, exploration: 0.056575091797066025, score: 10\n",
      "Run: 58, exploration: 0.052741004581916356, score: 15\n",
      "Run: 59, exploration: 0.05092252885731386, score: 8\n",
      "Run: 60, exploration: 0.04867631463831842, score: 10\n",
      "Run: 61, exploration: 0.04652918187562211, score: 10\n",
      "Run: 62, exploration: 0.04447676004441063, score: 10\n",
      "Run: 63, exploration: 0.04294323012606958, score: 8\n",
      "Run: 64, exploration: 0.04104898613852031, score: 10\n",
      "Run: 65, exploration: 0.03923829805195549, score: 10\n",
      "Run: 66, exploration: 0.03549537564593805, score: 21\n",
      "Run: 67, exploration: 0.03392965965983891, score: 10\n",
      "Run: 68, exploration: 0.03275978694079333, score: 8\n",
      "Run: 69, exploration: 0.03147209942303359, score: 9\n",
      "Run: 70, exploration: 0.021075222784267326, score: 81\n",
      "Run: 71, exploration: 0.020348562734319765, score: 8\n",
      "Run: 72, exploration: 0.01964695745288379, score: 8\n",
      "Run: 73, exploration: 0.018780321875645996, score: 10\n",
      "Run: 74, exploration: 0.012895378303744804, score: 76\n",
      "Run: 75, exploration: 0.012326557729527843, score: 10\n",
      "Run: 76, exploration: 0.01184203826198843, score: 9\n",
      "Run: 77, exploration: 0.01137656378004644, score: 9\n",
      "Run: 78, exploration: 0.01, score: 73\n",
      "Run: 79, exploration: 0.01, score: 53\n",
      "Run: 80, exploration: 0.01, score: 54\n",
      "Run: 81, exploration: 0.01, score: 65\n",
      "Run: 82, exploration: 0.01, score: 107\n",
      "Run: 83, exploration: 0.01, score: 9\n",
      "Run: 84, exploration: 0.01, score: 9\n",
      "Run: 85, exploration: 0.01, score: 11\n",
      "Run: 86, exploration: 0.01, score: 111\n",
      "Run: 87, exploration: 0.01, score: 67\n",
      "Run: 88, exploration: 0.01, score: 11\n",
      "Run: 89, exploration: 0.01, score: 10\n",
      "Run: 90, exploration: 0.01, score: 11\n",
      "Run: 91, exploration: 0.01, score: 43\n",
      "Run: 92, exploration: 0.01, score: 9\n",
      "Run: 93, exploration: 0.01, score: 10\n",
      "Run: 94, exploration: 0.01, score: 9\n",
      "Run: 95, exploration: 0.01, score: 115\n",
      "Run: 96, exploration: 0.01, score: 101\n",
      "Run: 97, exploration: 0.01, score: 10\n",
      "Run: 98, exploration: 0.01, score: 41\n",
      "Run: 99, exploration: 0.01, score: 9\n",
      "Run: 100, exploration: 0.01, score: 10\n",
      "Run: 1, score: 154\n",
      "Run: 2, score: 135\n",
      "Run: 3, score: 153\n",
      "Run: 4, score: 193\n",
      "Run: 5, score: 145\n",
      "Run: 6, score: 191\n",
      "Run: 7, score: 207\n",
      "Run: 8, score: 153\n",
      "Run: 9, score: 133\n",
      "Run: 10, score: 253\n",
      "Run: 11, score: 200\n",
      "Run: 12, score: 278\n",
      "Run: 13, score: 193\n",
      "Run: 14, score: 197\n",
      "Run: 15, score: 216\n",
      "Run: 16, score: 188\n",
      "Run: 17, score: 174\n",
      "Run: 18, score: 142\n",
      "Run: 19, score: 191\n",
      "Run: 20, score: 143\n",
      "Run: 21, score: 207\n",
      "Run: 22, score: 309\n",
      "Run: 23, score: 214\n",
      "Run: 24, score: 117\n",
      "Run: 25, score: 157\n",
      "Run: 26, score: 126\n",
      "Run: 27, score: 155\n",
      "Run: 28, score: 192\n",
      "Run: 29, score: 233\n",
      "Run: 30, score: 204\n",
      "Run: 31, score: 147\n",
      "Run: 32, score: 183\n",
      "Run: 33, score: 162\n",
      "Run: 34, score: 153\n",
      "Run: 35, score: 160\n",
      "Run: 36, score: 208\n",
      "Run: 37, score: 164\n",
      "Run: 38, score: 230\n",
      "Run: 39, score: 155\n",
      "Run: 40, score: 186\n",
      "Run: 41, score: 144\n",
      "Run: 42, score: 27\n",
      "Run: 43, score: 240\n",
      "Run: 44, score: 160\n",
      "Run: 45, score: 163\n",
      "Run: 46, score: 187\n",
      "Run: 47, score: 160\n",
      "Run: 48, score: 155\n",
      "Run: 49, score: 195\n",
      "Run: 50, score: 237\n",
      "Run: 51, score: 15\n",
      "Run: 52, score: 193\n",
      "Run: 53, score: 128\n",
      "Run: 54, score: 127\n",
      "Run: 55, score: 210\n",
      "Run: 56, score: 242\n",
      "Run: 57, score: 295\n",
      "Run: 58, score: 199\n",
      "Run: 59, score: 232\n",
      "Run: 60, score: 9\n",
      "Run: 61, score: 9\n",
      "Run: 62, score: 11\n",
      "Run: 63, score: 19\n",
      "Run: 64, score: 70\n",
      "Run: 65, score: 257\n",
      "Run: 66, score: 274\n",
      "Run: 67, score: 250\n",
      "Run: 68, score: 306\n",
      "Run: 69, score: 210\n",
      "Run: 70, score: 151\n",
      "Run: 71, score: 123\n",
      "Run: 72, score: 171\n",
      "Run: 73, score: 259\n",
      "Run: 74, score: 45\n",
      "Run: 75, score: 13\n",
      "Run: 76, score: 305\n",
      "Run: 77, score: 281\n",
      "Run: 78, score: 72\n",
      "Run: 79, score: 18\n",
      "Run: 80, score: 187\n",
      "Run: 81, score: 258\n",
      "Run: 82, score: 229\n",
      "Run: 83, score: 365\n",
      "Run: 84, score: 190\n",
      "Run: 85, score: 276\n",
      "Run: 86, score: 178\n",
      "Run: 87, score: 221\n",
      "Run: 88, score: 111\n",
      "Run: 89, score: 52\n",
      "Run: 90, score: 135\n",
      "Run: 91, score: 422\n",
      "Run: 92, score: 239\n",
      "Run: 93, score: 211\n",
      "Run: 94, score: 216\n",
      "Run: 95, score: 152\n",
      "Run: 96, score: 192\n",
      "Run: 97, score: 218\n",
      "Run: 98, score: 133\n",
      "Run: 99, score: 164\n",
      "Run: 100, score: 199\n"
     ]
    }
   ],
   "source": [
    "class TrainSolver:\n",
    "\n",
    "    def __init__(self, max_episodes):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.score_table = deque(maxlen=400)\n",
    "        self.average_of_last_runs = None\n",
    "        self.model = None\n",
    "        self.play_episodes = 100\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        self.solver = Network(observation_space, action_space)\n",
    "\n",
    "    def train(self):\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Solver starts\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "        self.model = self.solver.get_model()\n",
    "        episode = 0\n",
    "        score_eval = ScoreEvaluator(400, 50, self.model)\n",
    "        with open(\"Cartpole_DQN_extended_train_scores.csv\", \"w\") as csvfile:\n",
    "            header = [\"episode\", \"epsilon\", \"score\"]\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            n = 0\n",
    "            action = None\n",
    "            while episode < self.max_episodes:\n",
    "                episode += 1\n",
    "                state = env.reset()\n",
    "                state = np.reshape(state, [1, observation_space])\n",
    "                step = 0\n",
    "                while True:\n",
    "                    step += 1\n",
    "                    if n == 0:\n",
    "                        action, n_yes_or_no = self.solver.take_action(state)\n",
    "                        if n_yes_or_no:\n",
    "                            n = np.random.randint(0, 50)\n",
    "                    else:\n",
    "                        action = action\n",
    "                        n -= 1\n",
    "                    state_next, reward, done, info = env.step(action)\n",
    "                    if not done:\n",
    "                        reward = reward\n",
    "                    else:\n",
    "                        reward = -reward\n",
    "                    state_next = np.reshape(state_next, [1, observation_space])\n",
    "                    self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                    state = state_next\n",
    "\n",
    "                    if done:\n",
    "                        print(\"Run: \" + str(episode) + \", exploration: \" + str(self.solver.exploration_rate) + \", score: \" + str(step))\n",
    "                        writer.writerow([episode, self.solver.exploration_rate, step])\n",
    "                        self.score_table.append([episode, step])\n",
    "                        score_eval.store_score(episode, step)\n",
    "                        break\n",
    "                    self.solver.experience_replay()\n",
    "\n",
    "        train_scores = score_eval.evaluation_score(\"100 trains\")\n",
    "    \n",
    "    def return_trained_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def play(self, play_episodes=100, load_model=False, model_weights_dir=None, trained_model=None):\n",
    "\n",
    "        self.play_episodes = play_episodes\n",
    "        if load_model is not False:\n",
    "            if model_weights_dir is None:\n",
    "                print(\"Can't load specified model\")\n",
    "            elif trained_model is None:\n",
    "                print(\"Please pass a valid model as a parameter\")\n",
    "            else:\n",
    "                model = trained_model\n",
    "                model.load(model_weights_dir)\n",
    "        else:\n",
    "            model = self.model\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        episode = 0\n",
    "        score_eval = ScoreEvaluator(400, 100, model)\n",
    "        while episode < self.play_episodes:\n",
    "\n",
    "            episode += 1\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, observation_space])\n",
    "            step = 0\n",
    "            while True:\n",
    "                step += 1\n",
    "                action, ignore_bool = self.solver.take_action(state)\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "\n",
    "                if not done:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -reward\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "                self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print(\"Run: \" + str(episode) + \", score: \" + str(\n",
    "                        step))\n",
    "                    self.score_table.append([episode, step])\n",
    "                    score_eval.store_score(episode, step)\n",
    "                    break\n",
    "                self.solver.experience_replay()\n",
    "        \n",
    "        play_scores = score_eval.evaluation_score(\"100 Plays\")\n",
    "        with open(\"Cartpole_DQN_extended_play_scores.csv\", \"w\") as csvfile:\n",
    "            header = [\"episode\", \"score\"]\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            for i in play_scores:\n",
    "                i = tuple(i)\n",
    "                episode, score = i[0], i[1]\n",
    "                writer.writerow([episode, score])\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.model.save('cartpole_dqn_extended_model.h5')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = TrainSolver(100)\n",
    "    trainer.train()\n",
    "    trainer.play(100)\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-venezuela",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
